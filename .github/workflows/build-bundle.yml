name: Build Threat Intel Bundle

on:
  schedule:
    - cron: "30 4 * * *"  # Daily at 04:30 UTC (after all collectors finish)
  workflow_dispatch:
    inputs:
      promote_release:
        description: "Publish a release bundle if all gates pass"
        type: boolean
        default: false
      external_labels_workflow:
        description: "Workflow file that produces ML external signals artifact (optional)"
        type: string
        default: ""
      external_labels_artifact:
        description: "Artifact name containing external ML signals (optional)"
        type: string
        default: ""
      external_labels_file:
        description: "NDJSON file name inside the external signals artifact"
        type: string
        default: "signature-ml-external-signals.ndjson"
      external_labels_sha256:
        description: "Expected SHA256 for the external signals file (optional)"
        type: string
        default: ""
      artifact_max_age_hours:
        description: "Max age (hours) for collector artifacts"
        type: string
        default: "30"
      allow_stale_artifacts:
        description: "Allow stale collector artifacts on workflow_dispatch"
        type: boolean
        default: false

permissions:
  contents: write
  actions: read

concurrency:
  group: build-threat-intel-bundle-${{ github.ref }}
  cancel-in-progress: false

jobs:
  build-bundle:
    runs-on: ubuntu-latest
    env:
      EGUARD_ML_TREND_RETENTION_DAYS: "90"
      EGUARD_PUBLISH_BUNDLE: ${{ github.event_name == 'workflow_dispatch' && inputs.promote_release && '1' || '0' }}
      EGUARD_ML_PUBLISH_MIN_PR_AUC: "0.70"
      EGUARD_ML_PUBLISH_MIN_ROC_AUC: "0.80"
      EGUARD_ML_PUBLISH_MAX_BRIER: "0.18"
      EGUARD_ML_PUBLISH_MAX_ECE: "0.12"
      EGUARD_ARTIFACT_MAX_AGE_HOURS: ${{ github.event_name == 'workflow_dispatch' && inputs.artifact_max_age_hours || '30' }}
      EGUARD_ARTIFACT_FAIL_ON_STALE: ${{ github.event_name == 'workflow_dispatch' && inputs.allow_stale_artifacts && '0' || '1' }}
      EGUARD_ML_EXTERNAL_SIGNALS_WORKFLOW: ${{ inputs.external_labels_workflow }}
      EGUARD_ML_EXTERNAL_SIGNALS_ARTIFACT: ${{ inputs.external_labels_artifact }}
      EGUARD_ML_EXTERNAL_SIGNALS_FILE: ${{ inputs.external_labels_file }}
      EGUARD_ML_EXTERNAL_SIGNALS_SHA256: ${{ inputs.external_labels_sha256 }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Rust toolchain
        uses: dtolnay/rust-toolchain@stable

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install dependencies
        run: pip install yara-python pyyaml pytest

      - name: Install zstd
        run: sudo apt-get update && sudo apt-get install -y zstd

      - name: Set version
        id: version
        run: echo "version=$(date -u +%Y.%m.%d.%H%M)" >> "$GITHUB_OUTPUT"

      - name: Validate collector artifact freshness
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          EGUARD_ARTIFACT_MAX_AGE_HOURS: ${{ env.EGUARD_ARTIFACT_MAX_AGE_HOURS }}
          EGUARD_ARTIFACT_FAIL_ON_STALE: ${{ env.EGUARD_ARTIFACT_FAIL_ON_STALE }}
          EGUARD_ARTIFACT_TARGET_BRANCH: ${{ github.ref_name }}
          EGUARD_ARTIFACT_FALLBACK_BRANCH: "main"
        run: |
          python3 - <<'PY'
          import json
          import os
          import subprocess
          import sys
          from datetime import datetime, timezone

          repo = os.environ["GITHUB_REPOSITORY"]
          max_age_hours = int(os.environ.get("EGUARD_ARTIFACT_MAX_AGE_HOURS", "30"))
          fail_on_stale = os.environ.get("EGUARD_ARTIFACT_FAIL_ON_STALE", "1").strip().lower() not in {"0", "false", "no"}
          target_branch = os.environ.get("EGUARD_ARTIFACT_TARGET_BRANCH", "").strip()
          fallback_branch = os.environ.get("EGUARD_ARTIFACT_FALLBACK_BRANCH", "").strip()
          required = [
              "sigma-filtered",
              "yara-collected",
              "ioc-curated",
              "cve-extracted",
              "suricata-collected",
              "elastic-rules",
          ]

          artifacts = []
          for page in range(1, 6):
              raw = subprocess.check_output(
                  ["gh", "api", f"repos/{repo}/actions/artifacts?per_page=100&page={page}"],
                  text=True,
              )
              page_artifacts = json.loads(raw).get("artifacts", [])
              if not page_artifacts:
                  break
              artifacts.extend(page_artifacts)

          def branch_priority(head_branch: str) -> int:
              if target_branch and head_branch == target_branch:
                  return 0
              if fallback_branch and head_branch == fallback_branch:
                  return 1
              if not target_branch and not fallback_branch:
                  return 0
              return 2

          latest = {}
          for artifact in artifacts:
              if artifact.get("expired"):
                  continue
              name = artifact.get("name")
              if name not in required:
                  continue

              workflow_run = artifact.get("workflow_run") or {}
              head_branch = str(workflow_run.get("head_branch") or "")
              if head_branch and branch_priority(head_branch) >= 2:
                  continue

              created = artifact.get("created_at", "")
              if not name or not created:
                  continue
              priority = branch_priority(head_branch)
              if (
                  name not in latest
                  or priority < latest[name]["_priority"]
                  or (priority == latest[name]["_priority"] and created > latest[name]["created_at"])
              ):
                  artifact["_priority"] = priority
                  latest[name] = artifact

          now = datetime.now(timezone.utc)
          failures = []

          for name in required:
              artifact = latest.get(name)
              if not artifact:
                  failures.append(f"missing artifact metadata: {name}")
                  continue

              created_at = datetime.fromisoformat(artifact["created_at"].replace("Z", "+00:00"))
              age_hours = (now - created_at).total_seconds() / 3600.0
              run_info = artifact.get("workflow_run") or {}
              print(
                  f"{name}: id={artifact.get('id')} run={run_info.get('id')} "
                  f"branch={run_info.get('head_branch')} created_at={artifact['created_at']} "
                  f"age_hours={age_hours:.2f}"
              )

              if age_hours > max_age_hours:
                  failures.append(
                      f"artifact too old: {name} age {age_hours:.2f}h > {max_age_hours}h"
                  )

          if failures:
              print("artifact freshness gate failed:")
              for failure in failures:
                  print(f"- {failure}")
              if fail_on_stale:
                  sys.exit(1)
              print("continuing despite stale artifacts (shadow mode)")

          selected = []
          for name in required:
              artifact = latest[name]
              run_info = artifact.get("workflow_run") or {}
              selected.append(
                  {
                      "name": name,
                      "artifact_id": artifact.get("id"),
                      "run_id": run_info.get("id"),
                      "head_branch": run_info.get("head_branch"),
                      "created_at": artifact.get("created_at"),
                      "selection_priority": artifact.get("_priority"),
                  }
              )

          with open("/tmp/collector-artifacts.json", "w", encoding="utf-8") as f:
              json.dump(
                  {
                      "target_branch": target_branch,
                      "required": required,
                      "selected_artifacts": selected,
                  },
                  f,
                  indent=2,
              )
              f.write("\n")

          print("artifact freshness gate passed")
          PY

      - name: Download latest collector artifacts (strict)
        run: |
          python3 - <<'PY'
          import json
          import os
          import shutil
          import subprocess
          import sys
          from pathlib import Path

          repo = os.environ["GITHUB_REPOSITORY"]
          required = [
              "sigma-filtered",
              "yara-collected",
              "ioc-curated",
              "cve-extracted",
              "suricata-collected",
              "elastic-rules",
          ]

          metadata_path = Path("/tmp/collector-artifacts.json")
          if not metadata_path.is_file():
              print("ERROR: missing artifact metadata from freshness gate")
              sys.exit(1)

          metadata = json.loads(metadata_path.read_text(encoding="utf-8"))
          selected = {item.get("name"): item for item in metadata.get("selected_artifacts", [])}

          root = Path("/tmp/artifacts")
          root.mkdir(parents=True, exist_ok=True)

          failures: list[str] = []
          for name in required:
              item = selected.get(name)
              if not item:
                  failures.append(f"missing selected artifact metadata: {name}")
                  continue

              run_id = item.get("run_id")
              if not run_id:
                  failures.append(f"artifact {name} missing run_id")
                  continue

              target = root / name
              if target.exists():
                  shutil.rmtree(target)
              target.mkdir(parents=True, exist_ok=True)

              print(f"Downloading artifact {name} from run {run_id}")
              cmd = [
                  "gh",
                  "run",
                  "download",
                  str(run_id),
                  "--repo",
                  repo,
                  "--name",
                  name,
                  "--dir",
                  str(target),
              ]
              result = subprocess.run(cmd, capture_output=True, text=True)
              if result.returncode != 0:
                  failures.append(
                      f"failed to download {name} from run {run_id}: {result.stderr.strip() or result.stdout.strip()}"
                  )

          if failures:
              print("artifact download step failed:")
              for failure in failures:
                  print(f"- {failure}")
              sys.exit(1)

          def file_count(path: Path) -> int:
              return sum(1 for p in path.rglob("*") if p.is_file())

          print("=== Artifacts downloaded ===")
          for name in required:
              print(f"  {name}: {file_count(root / name)} files")

          sigma_count = sum(1 for p in (root / "sigma-filtered").rglob("*") if p.is_file() and p.suffix.lower() in {".yml", ".yaml"})
          yara_count = sum(1 for p in (root / "yara-collected").rglob("*") if p.is_file() and p.suffix.lower() in {".yar", ".yara"})
          if sigma_count <= 0:
              failures.append("sigma-filtered artifact has no SIGMA rules")
          if yara_count <= 0:
              failures.append("yara-collected artifact has no YARA rules")

          for ioc_file in ("hashes.txt", "domains.txt", "ips.txt"):
              path = root / "ioc-curated" / ioc_file
              if not path.is_file() or path.stat().st_size <= 0:
                  failures.append(f"missing or empty IOC file: {ioc_file}")

          cve_path = root / "cve-extracted" / "cves.jsonl"
          if not cve_path.is_file() or cve_path.stat().st_size <= 0:
              failures.append("cve-extracted/cves.jsonl is missing or empty")

          if failures:
              print("artifact content validation failed:")
              for failure in failures:
                  print(f"- {failure}")
              sys.exit(1)

          print("artifact download + content validation passed")
          PY
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Validate and deduplicate YARA rules
        run: |
          mkdir -p /tmp/yara_validated
          YARA_DIR="/tmp/artifacts/yara-collected"
          if [ -d "$YARA_DIR" ] && [ "$(find "$YARA_DIR" -name '*.yar' -o -name '*.yara' 2>/dev/null | head -1)" ]; then
            python threat-intel/processing/yara_validate.py \
              --input "$YARA_DIR" \
              --output /tmp/yara_validated \
              --test-files threat-intel/tests/clean_files \
              --deduplicate \
              --source-priority yara-forge,elastic,gcti,reversinglabs,malpedia,bartblaze,inquest
          else
            echo "No YARA rules to validate"
          fi

      - name: Prepare IOC files
        run: |
          mkdir -p /tmp/ioc_ready
          IOC_DIR="/tmp/artifacts/ioc-curated"
          if [ -d "$IOC_DIR" ]; then
            cp "$IOC_DIR"/*.txt /tmp/ioc_ready/ 2>/dev/null || true
          fi

      - name: Build bundle
        run: |
          python threat-intel/processing/build_bundle.py \
            --sigma /tmp/artifacts/sigma-filtered \
            --yara /tmp/yara_validated \
            --ioc /tmp/ioc_ready \
            --cve /tmp/artifacts/cve-extracted/cves.jsonl \
            --suricata /tmp/artifacts/suricata-collected \
            --elastic /tmp/artifacts/elastic-rules \
            --output bundle \
            --version "${{ steps.version.outputs.version }}"

      - name: Enforce signature database coverage gate
        env:
          EGUARD_MIN_SIGMA: "150"
          EGUARD_MIN_YARA: "600"
          EGUARD_MIN_IOC_HASH: "1000"
          EGUARD_MIN_IOC_DOMAIN: "300"
          EGUARD_MIN_IOC_IP: "1500"
          EGUARD_MIN_CVE: "1000"
          EGUARD_MIN_CVE_KEV: "50"
          EGUARD_MIN_SIGNATURE_TOTAL: "900"
          EGUARD_MIN_DATABASE_TOTAL: "5000"
          EGUARD_MIN_YARA_SOURCES: "3"
          EGUARD_MIN_SIGMA_SOURCES: "2"
          EGUARD_MIN_SURICATA: "1000"
          EGUARD_MIN_ELASTIC: "100"
          EGUARD_REQUIRE_SURICATA: "true"
          EGUARD_REQUIRE_ELASTIC: "true"
          EGUARD_REQUIRED_YARA_SOURCES: "yara-forge,gcti,reversinglabs"
          EGUARD_REQUIRED_SIGMA_SOURCES: "rules,rules-emerging-threats,rules-threat-hunting"
          EGUARD_MIN_YARA_SOURCE_RULES: "yara-forge=250,gcti=40,reversinglabs=40"
          EGUARD_MIN_SIGMA_SOURCE_RULES: "rules=100,rules-emerging-threats=20,rules-threat-hunting=20"
        run: |
          EXTRA_FLAGS=()
          if [ "${EGUARD_REQUIRE_SURICATA}" = "true" ]; then
            EXTRA_FLAGS+=(--require-suricata)
          fi
          if [ "${EGUARD_REQUIRE_ELASTIC}" = "true" ]; then
            EXTRA_FLAGS+=(--require-elastic)
          fi
          for src in ${EGUARD_REQUIRED_YARA_SOURCES//,/ }; do
            if [ -n "${src}" ]; then
              EXTRA_FLAGS+=(--require-yara-source "${src}")
            fi
          done
          for src in ${EGUARD_REQUIRED_SIGMA_SOURCES//,/ }; do
            if [ -n "${src}" ]; then
              EXTRA_FLAGS+=(--require-sigma-source "${src}")
            fi
          done
          for src_rule in ${EGUARD_MIN_YARA_SOURCE_RULES//,/ }; do
            if [ -n "${src_rule}" ]; then
              EXTRA_FLAGS+=(--min-yara-source-rules "${src_rule}")
            fi
          done
          for src_rule in ${EGUARD_MIN_SIGMA_SOURCE_RULES//,/ }; do
            if [ -n "${src_rule}" ]; then
              EXTRA_FLAGS+=(--min-sigma-source-rules "${src_rule}")
            fi
          done

          python threat-intel/processing/bundle_coverage_gate.py \
            --manifest bundle/manifest.json \
            --output bundle/coverage-metrics.json \
            --min-sigma "${EGUARD_MIN_SIGMA}" \
            --min-yara "${EGUARD_MIN_YARA}" \
            --min-ioc-hash "${EGUARD_MIN_IOC_HASH}" \
            --min-ioc-domain "${EGUARD_MIN_IOC_DOMAIN}" \
            --min-ioc-ip "${EGUARD_MIN_IOC_IP}" \
            --min-cve "${EGUARD_MIN_CVE}" \
            --min-cve-kev "${EGUARD_MIN_CVE_KEV}" \
            --min-signature-total "${EGUARD_MIN_SIGNATURE_TOTAL}" \
            --min-database-total "${EGUARD_MIN_DATABASE_TOTAL}" \
            --min-yara-sources "${EGUARD_MIN_YARA_SOURCES}" \
            --min-sigma-sources "${EGUARD_MIN_SIGMA_SOURCES}" \
            --min-suricata "${EGUARD_MIN_SURICATA}" \
            --min-elastic "${EGUARD_MIN_ELASTIC}" \
            "${EXTRA_FLAGS[@]}"

      - name: Enforce ATT&CK coverage gate
        env:
          EGUARD_MIN_ATTACK_TECHNIQUES: "80"
          EGUARD_MIN_ATTACK_TACTICS: "10"
          EGUARD_MIN_SIGMA_ATTACK_RULES: "150"
          EGUARD_MIN_ELASTIC_ATTACK_RULES: "50"
          EGUARD_MIN_SIGMA_ATTACK_TECHNIQUES: "60"
          EGUARD_MIN_ELASTIC_ATTACK_TECHNIQUES: "20"
          EGUARD_REQUIRED_ATTACK_TACTICS: "execution,persistence,privilege_escalation,defense_evasion,credential_access,discovery,lateral_movement,collection,command_and_control,impact"
        run: |
          EXTRA_FLAGS=()
          for tactic in ${EGUARD_REQUIRED_ATTACK_TACTICS//,/ }; do
            if [ -n "${tactic}" ]; then
              EXTRA_FLAGS+=(--require-tactic "${tactic}")
            fi
          done

          python threat-intel/processing/attack_coverage_gate.py \
            --sigma-dir bundle/sigma \
            --elastic-jsonl bundle/elastic/elastic-rules.jsonl \
            --output bundle/attack-coverage.json \
            --min-techniques "${EGUARD_MIN_ATTACK_TECHNIQUES}" \
            --min-tactics "${EGUARD_MIN_ATTACK_TACTICS}" \
            --min-sigma-rules-with-attack "${EGUARD_MIN_SIGMA_ATTACK_RULES}" \
            --min-elastic-rules-with-attack "${EGUARD_MIN_ELASTIC_ATTACK_RULES}" \
            --min-sigma-techniques "${EGUARD_MIN_SIGMA_ATTACK_TECHNIQUES}" \
            --min-elastic-techniques "${EGUARD_MIN_ELASTIC_ATTACK_TECHNIQUES}" \
            "${EXTRA_FLAGS[@]}"

      - name: Download previous coverage baseline (best effort)
        run: |
          rm -f \
            /tmp/coverage-metrics.json \
            /tmp/previous-coverage-metrics.json \
            /tmp/attack-coverage.json \
            /tmp/previous-attack-coverage.json \
            /tmp/attack-critical-technique-gate.json \
            /tmp/previous-attack-critical-technique-gate.json \
            /tmp/attack-critical-regression-history.ndjson \
            /tmp/previous-attack-critical-regression-history.ndjson \
            /tmp/attack-burndown-scoreboard.json \
            /tmp/previous-attack-burndown-scoreboard.json \
            /tmp/signature-ml-readiness.json \
            /tmp/previous-signature-ml-readiness.json \
            /tmp/signature-ml-readiness-trend.ndjson \
            /tmp/previous-signature-ml-readiness-trend.ndjson \
            /tmp/signature-ml-offline-eval-report.json \
            /tmp/previous-signature-ml-offline-eval-report.json \
            /tmp/signature-ml-offline-eval-trend.ndjson \
            /tmp/previous-signature-ml-offline-eval-trend.ndjson
          rm -rf /tmp/previous-bundle-signature-coverage /tmp/previous-bundle-signature-coverage.zip

          current_run_id="${GITHUB_RUN_ID:-}"
          workflow_id="$(gh api "repos/${GITHUB_REPOSITORY}/actions/workflows/build-bundle.yml" --jq '.id' 2>/dev/null || true)"
          previous_run_id=""
          if [ -n "${workflow_id}" ]; then
            previous_run_id="$(gh api "repos/${GITHUB_REPOSITORY}/actions/workflows/${workflow_id}/runs?status=success&per_page=10" \
              --jq '.workflow_runs[] | select(.id != '"${current_run_id:-0}"') | .id' | head -n1)"
          fi

          if [ -n "${previous_run_id}" ]; then
            artifact_id="$(gh api "repos/${GITHUB_REPOSITORY}/actions/runs/${previous_run_id}/artifacts" \
              --jq '.artifacts[] | select(.name=="bundle-signature-coverage") | .id' | head -n1)"
            if [ -n "${artifact_id}" ]; then
              gh api "repos/${GITHUB_REPOSITORY}/actions/artifacts/${artifact_id}/zip" > /tmp/previous-bundle-signature-coverage.zip
              unzip -q /tmp/previous-bundle-signature-coverage.zip -d /tmp/previous-bundle-signature-coverage
              echo "Downloaded bundle-signature-coverage artifact from run ${previous_run_id}"
            else
              echo "No bundle-signature-coverage artifact found for run ${previous_run_id}"
            fi
          else
            echo "No previous successful build-bundle run found for artifact baseline"
          fi

          baseline_found=0
          copy_baseline() {
            local name="$1"
            local src=""
            if [ -f "/tmp/previous-bundle-signature-coverage/${name}" ]; then
              src="/tmp/previous-bundle-signature-coverage/${name}"
            elif [ -f "/tmp/previous-bundle-signature-coverage/bundle/${name}" ]; then
              src="/tmp/previous-bundle-signature-coverage/bundle/${name}"
            fi
            if [ -n "${src}" ]; then
              cp "${src}" "/tmp/previous-${name}"
              echo "Loaded previous baseline: ${name}"
              baseline_found=1
            fi
          }

          copy_baseline "coverage-metrics.json"
          copy_baseline "attack-coverage.json"
          copy_baseline "attack-critical-technique-gate.json"
          copy_baseline "attack-critical-regression-history.ndjson"
          copy_baseline "attack-burndown-scoreboard.json"
          copy_baseline "signature-ml-readiness.json"
          copy_baseline "signature-ml-readiness-trend.ndjson"
          copy_baseline "signature-ml-offline-eval-report.json"
          copy_baseline "signature-ml-offline-eval-trend.ndjson"

          if [ "${baseline_found}" -eq 1 ]; then
            echo "Using workflow artifact baseline; skipping release baseline download"
            exit 0
          fi

          LATEST_TAG="$(gh api "repos/${GITHUB_REPOSITORY}/releases?per_page=1" --jq '.[0].tag_name // ""' 2>/dev/null || true)"
          if [ -z "${LATEST_TAG}" ]; then
            echo "No previous release found; skipping regression baseline download"
            exit 0
          fi

          echo "Latest release tag: ${LATEST_TAG}"
          if gh release download "${LATEST_TAG}" --repo "${GITHUB_REPOSITORY}" --pattern "coverage-metrics.json" --dir /tmp 2>/dev/null; then
            if [ -f /tmp/coverage-metrics.json ]; then
              mv /tmp/coverage-metrics.json /tmp/previous-coverage-metrics.json
              echo "Downloaded previous coverage baseline"
            fi
          else
            echo "No previous coverage metrics asset found on ${LATEST_TAG}"
          fi

          if gh release download "${LATEST_TAG}" --repo "${GITHUB_REPOSITORY}" --pattern "attack-coverage.json" --dir /tmp 2>/dev/null; then
            if [ -f /tmp/attack-coverage.json ]; then
              mv /tmp/attack-coverage.json /tmp/previous-attack-coverage.json
              echo "Downloaded previous ATT&CK coverage baseline"
            fi
          else
            echo "No previous ATT&CK coverage asset found on ${LATEST_TAG}"
          fi

          if gh release download "${LATEST_TAG}" --repo "${GITHUB_REPOSITORY}" --pattern "attack-critical-technique-gate.json" --dir /tmp 2>/dev/null; then
            if [ -f /tmp/attack-critical-technique-gate.json ]; then
              mv /tmp/attack-critical-technique-gate.json /tmp/previous-attack-critical-technique-gate.json
              echo "Downloaded previous critical ATT&CK gate baseline"
            fi
          else
            echo "No previous critical ATT&CK gate asset found on ${LATEST_TAG}"
          fi

          if gh release download "${LATEST_TAG}" --repo "${GITHUB_REPOSITORY}" --pattern "attack-burndown-scoreboard.json" --dir /tmp 2>/dev/null; then
            if [ -f /tmp/attack-burndown-scoreboard.json ]; then
              mv /tmp/attack-burndown-scoreboard.json /tmp/previous-attack-burndown-scoreboard.json
              echo "Downloaded previous ATT&CK burn-down scoreboard baseline"
            fi
          else
            echo "No previous ATT&CK burn-down scoreboard asset found on ${LATEST_TAG}"
          fi

          if gh release download "${LATEST_TAG}" --repo "${GITHUB_REPOSITORY}" --pattern "attack-critical-regression-history.ndjson" --dir /tmp 2>/dev/null; then
            if [ -f /tmp/attack-critical-regression-history.ndjson ]; then
              mv /tmp/attack-critical-regression-history.ndjson /tmp/previous-attack-critical-regression-history.ndjson
              echo "Downloaded previous critical ATT&CK regression history"
            fi
          else
            echo "No previous critical ATT&CK regression history asset found on ${LATEST_TAG}"
          fi

          if gh release download "${LATEST_TAG}" --repo "${GITHUB_REPOSITORY}" --pattern "signature-ml-readiness.json" --dir /tmp 2>/dev/null; then
            if [ -f /tmp/signature-ml-readiness.json ]; then
              mv /tmp/signature-ml-readiness.json /tmp/previous-signature-ml-readiness.json
              echo "Downloaded previous signature ML readiness baseline"
            fi
          else
            echo "No previous signature ML readiness asset found on ${LATEST_TAG}"
          fi

          if gh release download "${LATEST_TAG}" --repo "${GITHUB_REPOSITORY}" --pattern "signature-ml-readiness-trend.ndjson" --dir /tmp 2>/dev/null; then
            if [ -f /tmp/signature-ml-readiness-trend.ndjson ]; then
              mv /tmp/signature-ml-readiness-trend.ndjson /tmp/previous-signature-ml-readiness-trend.ndjson
              echo "Downloaded previous signature ML readiness trend baseline"
            fi
          else
            echo "No previous signature ML readiness trend asset found on ${LATEST_TAG}"
          fi

          if gh release download "${LATEST_TAG}" --repo "${GITHUB_REPOSITORY}" --pattern "signature-ml-offline-eval-report.json" --dir /tmp 2>/dev/null; then
            if [ -f /tmp/signature-ml-offline-eval-report.json ]; then
              mv /tmp/signature-ml-offline-eval-report.json /tmp/previous-signature-ml-offline-eval-report.json
              echo "Downloaded previous signature ML offline eval baseline"
            fi
          else
            echo "No previous signature ML offline eval asset found on ${LATEST_TAG}"
          fi

          if gh release download "${LATEST_TAG}" --repo "${GITHUB_REPOSITORY}" --pattern "signature-ml-offline-eval-trend.ndjson" --dir /tmp 2>/dev/null; then
            if [ -f /tmp/signature-ml-offline-eval-trend.ndjson ]; then
              mv /tmp/signature-ml-offline-eval-trend.ndjson /tmp/previous-signature-ml-offline-eval-trend.ndjson
              echo "Downloaded previous signature ML offline eval trend baseline"
            fi
          else
            echo "No previous signature ML offline eval trend asset found on ${LATEST_TAG}"
          fi
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Enforce signature coverage regression gate
        env:
          EGUARD_MAX_DROP_SIGMA_PCT: "25"
          EGUARD_MAX_DROP_YARA_PCT: "25"
          EGUARD_MAX_DROP_SURICATA_PCT: "25"
          EGUARD_MAX_DROP_ELASTIC_PCT: "25"
          EGUARD_MAX_DROP_IOC_TOTAL_PCT: "25"
          EGUARD_MAX_DROP_CVE_PCT: "25"
          EGUARD_MAX_DROP_SIGNATURE_TOTAL_PCT: "15"
          EGUARD_MAX_DROP_DATABASE_TOTAL_PCT: "15"
          EGUARD_MAX_DROP_YARA_SOURCES: "1"
          EGUARD_MAX_DROP_SIGMA_SOURCES: "1"
        run: |
          python threat-intel/processing/coverage_regression_gate.py \
            --current bundle/coverage-metrics.json \
            --previous /tmp/previous-coverage-metrics.json \
            --output bundle/coverage-regression.json \
            --max-drop-sigma-pct "${EGUARD_MAX_DROP_SIGMA_PCT}" \
            --max-drop-yara-pct "${EGUARD_MAX_DROP_YARA_PCT}" \
            --max-drop-suricata-pct "${EGUARD_MAX_DROP_SURICATA_PCT}" \
            --max-drop-elastic-pct "${EGUARD_MAX_DROP_ELASTIC_PCT}" \
            --max-drop-ioc-total-pct "${EGUARD_MAX_DROP_IOC_TOTAL_PCT}" \
            --max-drop-cve-pct "${EGUARD_MAX_DROP_CVE_PCT}" \
            --max-drop-signature-total-pct "${EGUARD_MAX_DROP_SIGNATURE_TOTAL_PCT}" \
            --max-drop-database-total-pct "${EGUARD_MAX_DROP_DATABASE_TOTAL_PCT}" \
            --max-drop-yara-sources "${EGUARD_MAX_DROP_YARA_SOURCES}" \
            --max-drop-sigma-sources "${EGUARD_MAX_DROP_SIGMA_SOURCES}"

      - name: Enforce ATT&CK coverage regression gate
        env:
          EGUARD_MAX_DROP_TOTAL_TECHNIQUES_PCT: "20"
          EGUARD_MAX_DROP_TOTAL_TACTICS_PCT: "20"
          EGUARD_MAX_DROP_SIGMA_ATTACK_RULES_PCT: "25"
          EGUARD_MAX_DROP_ELASTIC_ATTACK_RULES_PCT: "25"
          EGUARD_MAX_DROP_SIGMA_ATTACK_TECHNIQUES_PCT: "25"
          EGUARD_MAX_DROP_ELASTIC_ATTACK_TECHNIQUES_PCT: "25"
        run: |
          python threat-intel/processing/attack_regression_gate.py \
            --current bundle/attack-coverage.json \
            --previous /tmp/previous-attack-coverage.json \
            --output bundle/attack-regression.json \
            --max-drop-total-techniques-pct "${EGUARD_MAX_DROP_TOTAL_TECHNIQUES_PCT}" \
            --max-drop-total-tactics-pct "${EGUARD_MAX_DROP_TOTAL_TACTICS_PCT}" \
            --max-drop-sigma-rules-with-attack-pct "${EGUARD_MAX_DROP_SIGMA_ATTACK_RULES_PCT}" \
            --max-drop-elastic-rules-with-attack-pct "${EGUARD_MAX_DROP_ELASTIC_ATTACK_RULES_PCT}" \
            --max-drop-sigma-techniques-pct "${EGUARD_MAX_DROP_SIGMA_ATTACK_TECHNIQUES_PCT}" \
            --max-drop-elastic-techniques-pct "${EGUARD_MAX_DROP_ELASTIC_ATTACK_TECHNIQUES_PCT}"

      - name: Enforce ATT&CK gap burn-down gate
        env:
          EGUARD_ATTACK_GOAL_TECHNIQUES: "120"
          EGUARD_ATTACK_GOAL_TACTICS: "12"
          EGUARD_ATTACK_REQUIRED_TACTICS: "execution,persistence,privilege_escalation,defense_evasion,credential_access,discovery,lateral_movement,collection,command_and_control,impact"
          EGUARD_ATTACK_MAX_TECHNIQUE_GAP_INCREASE: "0"
          EGUARD_ATTACK_MAX_TACTIC_GAP_INCREASE: "0"
          EGUARD_ATTACK_MAX_NEW_MISSING_TACTICS: "0"
        run: |
          EXTRA_FLAGS=()
          for tactic in ${EGUARD_ATTACK_REQUIRED_TACTICS//,/ }; do
            if [ -n "${tactic}" ]; then
              EXTRA_FLAGS+=(--require-tactic "${tactic}")
            fi
          done

          python threat-intel/processing/attack_gap_burndown_gate.py \
            --current bundle/attack-coverage.json \
            --previous /tmp/previous-attack-coverage.json \
            --output bundle/attack-gap-burndown.json \
            --goal-techniques "${EGUARD_ATTACK_GOAL_TECHNIQUES}" \
            --goal-tactics "${EGUARD_ATTACK_GOAL_TACTICS}" \
            --max-technique-gap-increase "${EGUARD_ATTACK_MAX_TECHNIQUE_GAP_INCREASE}" \
            --max-tactic-gap-increase "${EGUARD_ATTACK_MAX_TACTIC_GAP_INCREASE}" \
            --max-new-missing-required-tactics "${EGUARD_ATTACK_MAX_NEW_MISSING_TACTICS}" \
            "${EXTRA_FLAGS[@]}"

      - name: Enforce critical ATT&CK technique floor gate
        env:
          EGUARD_CRITICAL_TECHNIQUES_FILE: "threat-intel/processing/attack_critical_techniques.json"
          EGUARD_CRITICAL_MIN_COVERED: "20"
          EGUARD_CRITICAL_MIN_COVERED_RATIO: "0.65"
          EGUARD_CRITICAL_MAX_MISSING: "12"
          EGUARD_REQUIRED_CRITICAL_TECHNIQUES: "T1059,T1078,T1021,T1562,T1486"
        run: |
          EXTRA_FLAGS=()
          for technique in ${EGUARD_REQUIRED_CRITICAL_TECHNIQUES//,/ }; do
            if [ -n "${technique}" ]; then
              EXTRA_FLAGS+=(--require-technique "${technique}")
            fi
          done

          python threat-intel/processing/attack_critical_technique_gate.py \
            --attack-coverage bundle/attack-coverage.json \
            --critical-techniques "${EGUARD_CRITICAL_TECHNIQUES_FILE}" \
            --output bundle/attack-critical-technique-gate.json \
            --min-covered-count "${EGUARD_CRITICAL_MIN_COVERED}" \
            --min-covered-ratio "${EGUARD_CRITICAL_MIN_COVERED_RATIO}" \
            --max-missing-count "${EGUARD_CRITICAL_MAX_MISSING}" \
            "${EXTRA_FLAGS[@]}"

      - name: Enforce critical ATT&CK regression gate
        env:
          EGUARD_CRITICAL_MAX_COVERED_COUNT_DROP: "2"
          EGUARD_CRITICAL_MAX_COVERED_RATIO_DROP: "0.08"
          EGUARD_CRITICAL_MAX_MISSING_COUNT_INCREASE: "2"
          EGUARD_CRITICAL_MAX_MISSING_REQUIRED_INCREASE: "0"
          EGUARD_CRITICAL_MAX_P0_UNCOVERED_INCREASE: "0"
          EGUARD_CRITICAL_MAX_OWNER_P0_UNCOVERED_INCREASE: "1"
        run: |
          python threat-intel/processing/attack_critical_regression_gate.py \
            --current bundle/attack-critical-technique-gate.json \
            --previous /tmp/previous-attack-critical-technique-gate.json \
            --output bundle/attack-critical-regression.json \
            --max-covered-count-drop "${EGUARD_CRITICAL_MAX_COVERED_COUNT_DROP}" \
            --max-covered-ratio-drop "${EGUARD_CRITICAL_MAX_COVERED_RATIO_DROP}" \
            --max-missing-count-increase "${EGUARD_CRITICAL_MAX_MISSING_COUNT_INCREASE}" \
            --max-missing-required-increase "${EGUARD_CRITICAL_MAX_MISSING_REQUIRED_INCREASE}" \
            --max-p0-uncovered-increase "${EGUARD_CRITICAL_MAX_P0_UNCOVERED_INCREASE}" \
            --max-owner-p0-uncovered-increase "${EGUARD_CRITICAL_MAX_OWNER_P0_UNCOVERED_INCREASE}"

      - name: Update critical ATT&CK regression history
        env:
          EGUARD_CRITICAL_HISTORY_MAX_ENTRIES: "180"
        run: |
          python threat-intel/processing/update_attack_critical_regression_history.py \
            --current-report bundle/attack-critical-regression.json \
            --previous-history /tmp/previous-attack-critical-regression-history.ndjson \
            --output-history bundle/attack-critical-regression-history.ndjson \
            --output-summary bundle/attack-critical-regression-history-summary.json \
            --max-entries "${EGUARD_CRITICAL_HISTORY_MAX_ENTRIES}"

      - name: Enforce critical ATT&CK owner streak gate
        env:
          EGUARD_CRITICAL_OWNER_STREAK_WINDOW_SIZE: "10"
          EGUARD_CRITICAL_OWNER_STREAK_MIN_HISTORY: "3"
          EGUARD_CRITICAL_MAX_CONSEC_OWNER_REGRESSION: "2"
        run: |
          python threat-intel/processing/attack_critical_owner_streak_gate.py \
            --history bundle/attack-critical-regression-history.ndjson \
            --output bundle/attack-critical-owner-streak-gate.json \
            --window-size "${EGUARD_CRITICAL_OWNER_STREAK_WINDOW_SIZE}" \
            --min-history-length "${EGUARD_CRITICAL_OWNER_STREAK_MIN_HISTORY}" \
            --max-consecutive-owner-regression "${EGUARD_CRITICAL_MAX_CONSEC_OWNER_REGRESSION}"

      - name: Generate ATT&CK burn-down scoreboard
        run: |
          python threat-intel/processing/attack_burndown_scoreboard.py \
            --attack-coverage bundle/attack-coverage.json \
            --critical-techniques threat-intel/processing/attack_critical_techniques.json \
            --attack-gap bundle/attack-gap-burndown.json \
            --previous-scoreboard /tmp/previous-attack-burndown-scoreboard.json \
            --output-json bundle/attack-burndown-scoreboard.json \
            --output-md bundle/attack-burndown-scoreboard.md

      - name: Generate signature ML readiness report (shadow)
        env:
          EGUARD_ML_MIN_FINAL_SCORE: "88"
          EGUARD_ML_MAX_SCORE_DROP: "3"
          EGUARD_ML_FAIL_ON_THRESHOLD: "0"
          EGUARD_ML_FAIL_ON_SCORE_DROP: "0"
        run: |
          python threat-intel/processing/signature_ml_readiness_gate.py \
            --manifest bundle/manifest.json \
            --coverage bundle/coverage-metrics.json \
            --attack-coverage bundle/attack-coverage.json \
            --critical-gate bundle/attack-critical-technique-gate.json \
            --critical-regression bundle/attack-critical-regression.json \
            --critical-owner-streak bundle/attack-critical-owner-streak-gate.json \
            --burndown-scoreboard bundle/attack-burndown-scoreboard.json \
            --previous /tmp/previous-signature-ml-readiness.json \
            --output bundle/signature-ml-readiness.json \
            --min-final-score "${EGUARD_ML_MIN_FINAL_SCORE}" \
            --max-score-drop "${EGUARD_ML_MAX_SCORE_DROP}" \
            --fail-on-threshold "${EGUARD_ML_FAIL_ON_THRESHOLD}" \
            --fail-on-score-drop "${EGUARD_ML_FAIL_ON_SCORE_DROP}"

      - name: Generate signature ML readiness trend report (shadow)
        env:
          EGUARD_ML_TREND_MAX_SCORE_DROP: "2.5"
          EGUARD_ML_TREND_MAX_COMPONENT_DROP: "8"
          EGUARD_ML_TREND_MAX_TIER_DROP: "1"
          EGUARD_ML_TREND_MAX_CONSECUTIVE_ALERTS: "3"
          EGUARD_ML_TREND_FAIL_ON_REGRESSION: "0"
        run: |
          python threat-intel/processing/signature_ml_readiness_trend_gate.py \
            --current bundle/signature-ml-readiness.json \
            --previous-trend /tmp/previous-signature-ml-readiness-trend.ndjson \
            --output-trend bundle/signature-ml-readiness-trend.ndjson \
            --output-report bundle/signature-ml-readiness-trend-report.json \
            --max-score-drop "${EGUARD_ML_TREND_MAX_SCORE_DROP}" \
            --max-component-drop "${EGUARD_ML_TREND_MAX_COMPONENT_DROP}" \
            --max-tier-drop "${EGUARD_ML_TREND_MAX_TIER_DROP}" \
            --max-consecutive-alerts "${EGUARD_ML_TREND_MAX_CONSECUTIVE_ALERTS}" \
            --fail-on-regression "${EGUARD_ML_TREND_FAIL_ON_REGRESSION}"

      - name: Download external ML signals artifact (optional)
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          artifact="${EGUARD_ML_EXTERNAL_SIGNALS_ARTIFACT:-}"
          if [ -z "${artifact}" ]; then
            echo "No external ML signals artifact configured; skipping"
            exit 0
          fi

          workflow="${EGUARD_ML_EXTERNAL_SIGNALS_WORKFLOW:-}"
          if [ -z "${workflow}" ]; then
            workflow="ml-labels.yml"
          fi
          file="${EGUARD_ML_EXTERNAL_SIGNALS_FILE:-signature-ml-external-signals.ndjson}"

          workflow_id="$(gh api "repos/${GITHUB_REPOSITORY}/actions/workflows/${workflow}" --jq '.id' 2>/dev/null || true)"
          if [ -z "${workflow_id}" ]; then
            echo "External labels workflow not found (${workflow}); skipping"
            exit 0
          fi

          run_id="$(gh api "repos/${GITHUB_REPOSITORY}/actions/workflows/${workflow_id}/runs?status=success&per_page=1" --jq '.workflow_runs[0].id // ""' 2>/dev/null || true)"
          if [ -z "${run_id}" ]; then
            echo "No successful runs found for ${workflow}; skipping"
            exit 0
          fi

          artifact_id="$(gh api "repos/${GITHUB_REPOSITORY}/actions/runs/${run_id}/artifacts" --jq '.artifacts[] | select(.name=="'"${artifact}"'") | .id' | head -n1)"
          if [ -z "${artifact_id}" ]; then
            echo "Artifact ${artifact} not found in run ${run_id}; skipping"
            exit 0
          fi

          rm -rf /tmp/external-signals /tmp/external-signals.zip
          gh api "repos/${GITHUB_REPOSITORY}/actions/artifacts/${artifact_id}/zip" > /tmp/external-signals.zip
          unzip -q /tmp/external-signals.zip -d /tmp/external-signals

          if [ ! -f "/tmp/external-signals/${file}" ]; then
            echo "External signals file not found: ${file}"
            exit 1
          fi

          if [ -n "${EGUARD_ML_EXTERNAL_SIGNALS_SHA256:-}" ]; then
            echo "${EGUARD_ML_EXTERNAL_SIGNALS_SHA256}  /tmp/external-signals/${file}" | sha256sum -c -
          elif [ -f "/tmp/external-signals/${file}.sha256" ]; then
            (cd /tmp/external-signals && sha256sum -c "${file}.sha256")
          else
            echo "No external signals checksum provided; proceeding without checksum validation"
          fi

          echo "EGUARD_ML_EXTERNAL_SIGNALS_PATH=/tmp/external-signals/${file}" >> "$GITHUB_ENV"

      - name: Build signature ML training corpus
        run: |
          EXTRA_ARGS=()
          if [ -n "${EGUARD_ML_EXTERNAL_SIGNALS_PATH:-}" ] && [ -f "${EGUARD_ML_EXTERNAL_SIGNALS_PATH}" ]; then
            EXTRA_ARGS+=(--external-signals "${EGUARD_ML_EXTERNAL_SIGNALS_PATH}")
          fi
          python threat-intel/processing/signature_ml_build_training_corpus.py \
            --manifest bundle/manifest.json \
            --coverage bundle/coverage-metrics.json \
            --readiness bundle/signature-ml-readiness.json \
            --attack-coverage bundle/attack-coverage.json \
            --critical-gate bundle/attack-critical-technique-gate.json \
            --output-signals bundle/signature-ml-signals.ndjson \
            --output-summary bundle/signature-ml-training-corpus-summary.json \
            --sample-count 960 \
            --window-days 60 \
            "${EXTRA_ARGS[@]}"

      - name: Validate signature ML label quality (shadow)
        env:
          EGUARD_ML_LABEL_MIN_ADJUDICATED: "700"
          EGUARD_ML_LABEL_MIN_POSITIVE: "120"
          EGUARD_ML_LABEL_MIN_NEGATIVE: "260"
          EGUARD_ML_LABEL_MIN_UNIQUE_HOSTS: "80"
          EGUARD_ML_LABEL_MIN_UNIQUE_RULES: "140"
          EGUARD_ML_LABEL_MAX_UNRESOLVED_RATIO: "0.15"
          EGUARD_ML_LABEL_MAX_P95_LATENCY_DAYS: "6"
          EGUARD_ML_LABEL_FAIL_ON_THRESHOLD: "1"
        run: |
          python threat-intel/processing/signature_ml_label_quality_gate.py \
            --signals bundle/signature-ml-signals.ndjson \
            --output-report bundle/signature-ml-label-quality-report.json \
            --output-labels bundle/signature-ml-labels.ndjson \
            --min-adjudicated "${EGUARD_ML_LABEL_MIN_ADJUDICATED}" \
            --min-positive "${EGUARD_ML_LABEL_MIN_POSITIVE}" \
            --min-negative "${EGUARD_ML_LABEL_MIN_NEGATIVE}" \
            --min-unique-hosts "${EGUARD_ML_LABEL_MIN_UNIQUE_HOSTS}" \
            --min-unique-rules "${EGUARD_ML_LABEL_MIN_UNIQUE_RULES}" \
            --max-unresolved-ratio "${EGUARD_ML_LABEL_MAX_UNRESOLVED_RATIO}" \
            --max-p95-label-latency-days "${EGUARD_ML_LABEL_MAX_P95_LATENCY_DAYS}" \
            --fail-on-threshold "${EGUARD_ML_LABEL_FAIL_ON_THRESHOLD}"

      - name: Build signature ML feature snapshot (shadow)
        env:
          EGUARD_ML_FEATURE_MIN_ROWS: "700"
          EGUARD_ML_FEATURE_MIN_UNIQUE_HOSTS: "80"
          EGUARD_ML_FEATURE_MIN_UNIQUE_RULES: "140"
          EGUARD_ML_FEATURE_MAX_MISSING_RATIO: "0.05"
          EGUARD_ML_FEATURE_MIN_SPAN_DAYS: "30"
          EGUARD_ML_FEATURE_FAIL_ON_THRESHOLD: "1"
        run: |
          python threat-intel/processing/signature_ml_feature_snapshot_gate.py \
            --labels bundle/signature-ml-labels.ndjson \
            --output-features bundle/signature-ml-features.ndjson \
            --output-schema bundle/signature-ml-feature-schema.json \
            --output-report bundle/signature-ml-feature-snapshot-report.json \
            --min-rows "${EGUARD_ML_FEATURE_MIN_ROWS}" \
            --min-unique-hosts "${EGUARD_ML_FEATURE_MIN_UNIQUE_HOSTS}" \
            --min-unique-rules "${EGUARD_ML_FEATURE_MIN_UNIQUE_RULES}" \
            --max-missing-feature-ratio "${EGUARD_ML_FEATURE_MAX_MISSING_RATIO}" \
            --min-temporal-span-days "${EGUARD_ML_FEATURE_MIN_SPAN_DAYS}" \
            --fail-on-threshold "${EGUARD_ML_FEATURE_FAIL_ON_THRESHOLD}"

      - name: Train signature ML model artifact
        run: |
          VERSION="${{ steps.version.outputs.version }}"
          python threat-intel/processing/signature_ml_train_model.py \
            --dataset bundle/signature-ml-features.ndjson \
            --feature-schema bundle/signature-ml-feature-schema.json \
            --labels-report bundle/signature-ml-label-quality-report.json \
            --model-version "rules-${VERSION}.ml.v1" \
            --model-out bundle/signature-ml-model.json \
            --metadata-out bundle/signature-ml-model-metadata.json

      - name: Evaluate signature ML offline metrics (shadow)
        env:
          EGUARD_ML_EVAL_MIN_SAMPLES: "220"
          EGUARD_ML_EVAL_MIN_PRECISION: "0.22"
          EGUARD_ML_EVAL_MIN_RECALL: "0.80"
          EGUARD_ML_EVAL_MIN_PR_AUC: "0.60"
          EGUARD_ML_EVAL_MIN_ROC_AUC: "0.76"
          EGUARD_ML_EVAL_MAX_BRIER: "0.25"
          EGUARD_ML_EVAL_MAX_ECE: "0.22"
          EGUARD_ML_EVAL_MAX_PR_DROP: "0.15"
          EGUARD_ML_EVAL_MAX_ROC_DROP: "0.15"
          EGUARD_ML_EVAL_FAIL_ON_THRESHOLD: "1"
          EGUARD_ML_EVAL_FAIL_ON_REGRESSION: "1"
        run: |
          python threat-intel/processing/signature_ml_offline_eval_gate.py \
            --dataset bundle/signature-ml-features.ndjson \
            --model bundle/signature-ml-model.json \
            --previous-report /tmp/previous-signature-ml-offline-eval-report.json \
            --previous-trend /tmp/previous-signature-ml-offline-eval-trend.ndjson \
            --output-report bundle/signature-ml-offline-eval-report.json \
            --output-trend bundle/signature-ml-offline-eval-trend.ndjson \
            --threshold 0.20 \
            --auto-threshold 1 \
            --eval-ratio 0.35 \
            --min-eval-samples "${EGUARD_ML_EVAL_MIN_SAMPLES}" \
            --min-precision "${EGUARD_ML_EVAL_MIN_PRECISION}" \
            --min-recall "${EGUARD_ML_EVAL_MIN_RECALL}" \
            --min-pr-auc "${EGUARD_ML_EVAL_MIN_PR_AUC}" \
            --min-roc-auc "${EGUARD_ML_EVAL_MIN_ROC_AUC}" \
            --max-brier-score "${EGUARD_ML_EVAL_MAX_BRIER}" \
            --max-ece "${EGUARD_ML_EVAL_MAX_ECE}" \
            --max-pr-auc-drop "${EGUARD_ML_EVAL_MAX_PR_DROP}" \
            --max-roc-auc-drop "${EGUARD_ML_EVAL_MAX_ROC_DROP}" \
            --fail-on-threshold "${EGUARD_ML_EVAL_FAIL_ON_THRESHOLD}" \
            --fail-on-regression "${EGUARD_ML_EVAL_FAIL_ON_REGRESSION}"

      - name: Validate signature ML offline eval trend (shadow)
        env:
          EGUARD_ML_EVAL_TREND_MAX_PR_DROP: "0.15"
          EGUARD_ML_EVAL_TREND_MAX_ROC_DROP: "0.15"
          EGUARD_ML_EVAL_TREND_MAX_BRIER_INCREASE: "0.08"
          EGUARD_ML_EVAL_TREND_MAX_ECE_INCREASE: "0.10"
          EGUARD_ML_EVAL_TREND_MAX_THRESHOLD_DRIFT: "0.25"
          EGUARD_ML_EVAL_TREND_MAX_CONSEC_ALERTS: "3"
          EGUARD_ML_EVAL_TREND_WINDOW_SIZE: "8"
          EGUARD_ML_EVAL_TREND_MIN_PASS_RATE: "0.60"
          EGUARD_ML_EVAL_TREND_FAIL_ON_REGRESSION: "1"
        run: |
          python threat-intel/processing/signature_ml_offline_eval_trend_gate.py \
            --trend bundle/signature-ml-offline-eval-trend.ndjson \
            --output bundle/signature-ml-offline-eval-trend-report.json \
            --max-pr-auc-drop "${EGUARD_ML_EVAL_TREND_MAX_PR_DROP}" \
            --max-roc-auc-drop "${EGUARD_ML_EVAL_TREND_MAX_ROC_DROP}" \
            --max-brier-increase "${EGUARD_ML_EVAL_TREND_MAX_BRIER_INCREASE}" \
            --max-ece-increase "${EGUARD_ML_EVAL_TREND_MAX_ECE_INCREASE}" \
            --max-threshold-drift "${EGUARD_ML_EVAL_TREND_MAX_THRESHOLD_DRIFT}" \
            --max-consecutive-alerts "${EGUARD_ML_EVAL_TREND_MAX_CONSEC_ALERTS}" \
            --window-size "${EGUARD_ML_EVAL_TREND_WINDOW_SIZE}" \
            --min-window-pass-rate "${EGUARD_ML_EVAL_TREND_MIN_PASS_RATE}" \
            --fail-on-regression "${EGUARD_ML_EVAL_TREND_FAIL_ON_REGRESSION}"

      - name: Sign signature ML model artifact
        run: |
          TMP_KEY="$(mktemp /tmp/eguard-ml-model-key.XXXXXX.pem)"
          TMP_PUB="$(mktemp /tmp/eguard-ml-model-pub.XXXXXX.pem)"
          trap 'rm -f "${TMP_KEY}" "${TMP_PUB}"' EXIT
          printf '%s\n' "${THREAT_INTEL_ED25519_PRIVATE_KEY_PEM}" > "${TMP_KEY}"
          openssl pkey -in "${TMP_KEY}" -pubout -out "${TMP_PUB}"
          python threat-intel/processing/ed25519_sign.py \
            --input bundle/signature-ml-model.json \
            --output-sig bundle/signature-ml-model.json.sig
          cp "${TMP_PUB}" bundle/signature-ml-model.pub.pem
        env:
          THREAT_INTEL_ED25519_PRIVATE_KEY_PEM: ${{ secrets.THREAT_INTEL_ED25519_PRIVATE_KEY_PEM }}

      - name: Validate signature ML model registry contract (shadow)
        env:
          EGUARD_ML_REGISTRY_MIN_PR_AUC: "0.60"
          EGUARD_ML_REGISTRY_MIN_ROC_AUC: "0.76"
          EGUARD_ML_REGISTRY_FAIL_ON_THRESHOLD: "1"
        run: |
          python threat-intel/processing/signature_ml_model_registry_gate.py \
            --model-artifact bundle/signature-ml-model.json \
            --metadata bundle/signature-ml-model-metadata.json \
            --offline-eval bundle/signature-ml-offline-eval-report.json \
            --offline-eval-trend-report bundle/signature-ml-offline-eval-trend-report.json \
            --feature-schema bundle/signature-ml-feature-schema.json \
            --labels-report bundle/signature-ml-label-quality-report.json \
            --signature-file bundle/signature-ml-model.json.sig \
            --public-key-file bundle/signature-ml-model.pub.pem \
            --output bundle/signature-ml-model-registry.json \
            --min-pr-auc "${EGUARD_ML_REGISTRY_MIN_PR_AUC}" \
            --min-roc-auc "${EGUARD_ML_REGISTRY_MIN_ROC_AUC}" \
            --require-signed-model 1 \
            --verify-signature 1 \
            --require-offline-eval-trend-pass 1 \
            --fail-on-threshold "${EGUARD_ML_REGISTRY_FAIL_ON_THRESHOLD}"

      - name: Summarize ML regression budgets
        if: ${{ always() }}
        run: |
          python - <<'PY'
          import json
          import os
          from pathlib import Path

          summary_path = os.environ.get("GITHUB_STEP_SUMMARY")
          def load_json(path: str) -> dict:
              p = Path(path)
              if not p.is_file():
                  return {}
              try:
                  return json.loads(p.read_text(encoding="utf-8"))
              except Exception:
                  return {}

          readiness_trend = load_json("bundle/signature-ml-readiness-trend-report.json")
          offline_trend = load_json("bundle/signature-ml-offline-eval-trend-report.json")
          offline_eval = load_json("bundle/signature-ml-offline-eval-report.json")
          registry = load_json("bundle/signature-ml-model-registry.json")

          lines = ["## ML Regression Budgets"]
          if readiness_trend:
              lines.append(f"- Readiness trend status: {readiness_trend.get('status', 'n/a')}")
              lines.append(f"- Readiness score drop: {readiness_trend.get('score_drop', 'n/a')}")
          if offline_trend:
              deltas = offline_trend.get("deltas", {}) if isinstance(offline_trend.get("deltas", {}), dict) else {}
              lines.append(f"- PR-AUC drop: {deltas.get('pr_auc_drop', 'n/a')}")
              lines.append(f"- ROC-AUC drop: {deltas.get('roc_auc_drop', 'n/a')}")
              lines.append(f"- Brier increase: {deltas.get('brier_increase', 'n/a')}")
              lines.append(f"- ECE increase: {deltas.get('ece_increase', 'n/a')}")
              lines.append(f"- Threshold drift: {deltas.get('threshold_drift', 'n/a')}")
          if offline_eval:
              metrics = offline_eval.get("metrics", {}) if isinstance(offline_eval.get("metrics", {}), dict) else {}
              lines.append(f"- Offline precision: {metrics.get('precision', 'n/a')}")
              lines.append(f"- Offline recall: {metrics.get('recall', 'n/a')}")
          if registry:
              metrics = registry.get("offline_metrics", {}) if isinstance(registry.get("offline_metrics", {}), dict) else {}
              lines.append(f"- Registry PR-AUC: {metrics.get('pr_auc', 'n/a')}")
              lines.append(f"- Registry ROC-AUC: {metrics.get('roc_auc', 'n/a')}")

          publish_thresholds = [
              f"- Publish PR-AUC >= {os.environ.get('EGUARD_ML_PUBLISH_MIN_PR_AUC', 'n/a')}",
              f"- Publish ROC-AUC >= {os.environ.get('EGUARD_ML_PUBLISH_MIN_ROC_AUC', 'n/a')}",
              f"- Publish Brier <= {os.environ.get('EGUARD_ML_PUBLISH_MAX_BRIER', 'n/a')}",
              f"- Publish ECE <= {os.environ.get('EGUARD_ML_PUBLISH_MAX_ECE', 'n/a')}",
          ]
          lines.extend(publish_thresholds)

          output = "\n".join(lines) + "\n"
          if summary_path:
              with open(summary_path, "a", encoding="utf-8") as handle:
                  handle.write(output)
          else:
              print(output)
          PY

      - name: Evaluate publish readiness
        id: publish_gate
        run: |
          python - <<'PY'
          import json
          import os
          from pathlib import Path

          def load_json(path: str) -> dict:
              p = Path(path)
              if not p.is_file():
                  return {}
              try:
                  return json.loads(p.read_text(encoding="utf-8"))
              except Exception:
                  return {}

          readiness_trend = load_json("bundle/signature-ml-readiness-trend-report.json")
          offline_trend = load_json("bundle/signature-ml-offline-eval-trend-report.json")
          offline_eval = load_json("bundle/signature-ml-offline-eval-report.json")
          registry = load_json("bundle/signature-ml-model-registry.json")

          def status_ok(report: dict) -> bool:
              status = str(report.get("status", "")).lower()
              return status in {"pass", "pass_no_baseline"}

          metrics = offline_eval.get("metrics", {}) if isinstance(offline_eval.get("metrics", {}), dict) else {}
          min_pr_auc = float(os.environ.get("EGUARD_ML_PUBLISH_MIN_PR_AUC", "0.0"))
          min_roc_auc = float(os.environ.get("EGUARD_ML_PUBLISH_MIN_ROC_AUC", "0.0"))
          max_brier = float(os.environ.get("EGUARD_ML_PUBLISH_MAX_BRIER", "1.0"))
          max_ece = float(os.environ.get("EGUARD_ML_PUBLISH_MAX_ECE", "1.0"))

          pr_auc = float(metrics.get("pr_auc", 0.0))
          roc_auc = float(metrics.get("roc_auc", 0.0))
          brier = float(metrics.get("brier_score", 1.0))
          ece = float(metrics.get("ece", 1.0))

          quality_ok = (
              pr_auc >= min_pr_auc
              and roc_auc >= min_roc_auc
              and brier <= max_brier
              and ece <= max_ece
          )

          publish_ready = (
              status_ok(readiness_trend)
              and status_ok(offline_trend)
              and str(registry.get("status", "")).lower() == "pass"
              and quality_ok
          )

          with open(os.environ["GITHUB_OUTPUT"], "a", encoding="utf-8") as handle:
              handle.write(f"ready={'1' if publish_ready else '0'}\n")

          print(
              "publish_ready="
              f"{publish_ready} pr_auc={pr_auc} roc_auc={roc_auc} brier={brier} ece={ece}"
          )
          PY

      - name: Run threat-intel tests
        run: pytest -q threat-intel/tests/test_bundle.py
        env:
          BUNDLE_DIR: bundle

      - name: Upload signature coverage metrics
        uses: actions/upload-artifact@v4
        with:
          name: bundle-signature-coverage
          path: |
            bundle/coverage-metrics.json
            bundle/coverage-regression.json
            bundle/attack-coverage.json
            bundle/attack-regression.json
            bundle/attack-gap-burndown.json
            bundle/attack-critical-technique-gate.json
            bundle/attack-critical-regression.json
            bundle/attack-critical-regression-history.ndjson
            bundle/attack-critical-regression-history-summary.json
            bundle/attack-critical-owner-streak-gate.json
            bundle/attack-burndown-scoreboard.json
            bundle/attack-burndown-scoreboard.md
            bundle/signature-ml-readiness.json
            bundle/signature-ml-readiness-trend.ndjson
            bundle/signature-ml-readiness-trend-report.json
            bundle/signature-ml-training-corpus-summary.json
            bundle/signature-ml-signals.ndjson
            bundle/signature-ml-label-quality-report.json
            bundle/signature-ml-labels.ndjson
            bundle/signature-ml-feature-snapshot-report.json
            bundle/signature-ml-features.ndjson
            bundle/signature-ml-feature-schema.json
            bundle/signature-ml-model.json
            bundle/signature-ml-model.json.sig
            bundle/signature-ml-model.pub.pem
            bundle/signature-ml-model-metadata.json
            bundle/signature-ml-offline-eval-report.json
            bundle/signature-ml-offline-eval-trend.ndjson
            bundle/signature-ml-offline-eval-trend-report.json
            bundle/signature-ml-model-registry.json
          retention-days: ${{ env.EGUARD_ML_TREND_RETENTION_DAYS }}

      - name: Package bundle
        run: |
          VERSION="${{ steps.version.outputs.version }}"
          BUNDLE="eguard-rules-${VERSION}.bundle.tar.zst"

          if [ -z "${THREAT_INTEL_ED25519_PRIVATE_KEY_PEM:-}" ]; then
            echo "Missing THREAT_INTEL_ED25519_PRIVATE_KEY_PEM secret"
            exit 1
          fi

          tar cf - -C bundle/ . | zstd -3 -o "${BUNDLE}"
          python threat-intel/processing/ed25519_sign.py \
            --input "${BUNDLE}" \
            --output-sig "${BUNDLE}.sig" \
            --public-key-hex-out "${BUNDLE}.pub.hex"

          TMP_KEY="$(mktemp /tmp/eguard-ed25519-key.XXXXXX.pem)"
          TMP_PUB="$(mktemp /tmp/eguard-ed25519-pub.XXXXXX.pem)"
          trap 'rm -f "${TMP_KEY}" "${TMP_PUB}"' EXIT
          printf '%s\n' "${THREAT_INTEL_ED25519_PRIVATE_KEY_PEM}" > "${TMP_KEY}"
          openssl pkey -in "${TMP_KEY}" -pubout -out "${TMP_PUB}"
          python threat-intel/processing/ed25519_verify.py \
            --input "${BUNDLE}" \
            --signature "${BUNDLE}.sig" \
            --public-key-file "${TMP_PUB}"

          echo "Bundle size: $(du -sh "${BUNDLE}" | cut -f1)"
        env:
          THREAT_INTEL_ED25519_PRIVATE_KEY_PEM: ${{ secrets.THREAT_INTEL_ED25519_PRIVATE_KEY_PEM }}

      - name: Verify agent can ingest generated bundle output
        run: |
          VERSION="${{ steps.version.outputs.version }}"
          BUNDLE="eguard-rules-${VERSION}.bundle.tar.zst"
          BUNDLE_ABS="${GITHUB_WORKSPACE}/${BUNDLE}"
          BUNDLE_PUBHEX="$(tr -d '\r\n' < "${BUNDLE}.pub.hex")"
          EGUARD_CI_BUNDLE_PATH="${BUNDLE_ABS}" \
          EGUARD_CI_BUNDLE_PUBHEX="${BUNDLE_PUBHEX}" \
            bash scripts/run_agent_bundle_ingestion_contract_ci.sh \
              --test-selector lifecycle::tests::load_bundle_rules_reads_ci_generated_signed_bundle
          EGUARD_CI_BUNDLE_PATH="${BUNDLE_ABS}" \
          EGUARD_CI_BUNDLE_PUBHEX="${BUNDLE_PUBHEX}" \
            bash scripts/run_agent_bundle_ingestion_contract_ci.sh \
              --test-selector lifecycle::tests::load_bundle_rules_rejects_tampered_ci_generated_signed_bundle

      - name: Verify agent loads ML model from bundle (full 7-layer ingestion)
        run: |
          VERSION="${{ steps.version.outputs.version }}"
          BUNDLE="eguard-rules-${VERSION}.bundle.tar.zst"
          BUNDLE_ABS="${GITHUB_WORKSPACE}/${BUNDLE}"
          BUNDLE_PUBHEX="$(tr -d '\r\n' < "${BUNDLE}.pub.hex")"
          EGUARD_CI_BUNDLE_PATH="${BUNDLE_ABS}" \
          EGUARD_CI_BUNDLE_PUBHEX="${BUNDLE_PUBHEX}" \
            bash scripts/run_agent_bundle_ingestion_contract_ci.sh \
              --test-selector lifecycle::tests::load_bundle_full_loads_ml_model_from_ci_generated_bundle

      - name: Validate ML model runtime compatibility
        run: |
          echo "Validating CI-trained ML model can be loaded by agent runtime..."
          python3 - <<'PYCHECK'
          import json, sys
          model = json.load(open("bundle/signature-ml-model.json"))
          required = ["suite", "model_type", "model_version", "features", "weights", "bias"]
          missing = [k for k in required if k not in model]
          if missing:
              print(f"FAIL: ML model missing required fields: {missing}")
              sys.exit(1)
          if model.get("suite") != "signature_ml_linear_logit_model":
              print(f"FAIL: unexpected suite: {model.get('suite')}")
              sys.exit(1)
          feature_count = len(model.get("features", []))
          weight_count = len(model.get("weights", {}))
          if feature_count == 0:
              print("FAIL: ML model has 0 features")
              sys.exit(1)
          if weight_count != feature_count:
              print(f"FAIL: feature/weight mismatch: {feature_count} features vs {weight_count} weights")
              sys.exit(1)
          # Verify the 20 runtime features are present
          RUNTIME_FEATURES = [
              "z1_ioc_hit", "z2_temporal_count", "z3_anomaly_high", "z3_anomaly_med",
              "z4_killchain_count", "yara_hit_count", "string_sig_count", "event_class_risk",
              "uid_is_root", "dst_port_risk", "has_command_line", "cmdline_length_norm",
              "prefilter_hit", "multi_layer_count",
              "cmdline_renyi_h2", "cmdline_compression", "cmdline_min_entropy", "cmdline_entropy_gap",
              "dns_entropy", "event_size_norm",
          ]
          model_features = set(model.get("features", []))
          covered = [f for f in RUNTIME_FEATURES if f in model_features]
          missing_runtime = [f for f in RUNTIME_FEATURES if f not in model_features]
          print(f"ML model: {model.get('model_version')}")
          print(f"  Features: {feature_count}")
          print(f"  Runtime features covered: {len(covered)}/{len(RUNTIME_FEATURES)}")
          if missing_runtime:
              print(f"FAIL: ML model missing runtime features: {missing_runtime}")
              sys.exit(1)
          threshold = model.get("threshold")
          if threshold is None or not isinstance(threshold, (int, float)):
              print("FAIL: ML model missing numeric threshold")
              sys.exit(1)
          if threshold < 0 or threshold > 1:
              print(f"FAIL: ML model threshold out of range: {threshold}")
              sys.exit(1)
          print(f"  Training samples: {model.get('training_samples', 'n/a')}")
          print(f"  Bias: {model.get('bias', 'n/a')}")
          print(f"  Threshold: {threshold}")
          print("ML model runtime compatibility: PASS")
          PYCHECK

      - name: Create GitHub Release
        if: ${{ env.EGUARD_PUBLISH_BUNDLE == '1' && steps.publish_gate.outputs.ready == '1' }}
        run: |
          VERSION="$(python3 -c 'import json; print(json.load(open("bundle/manifest.json")).get("version", "unknown"))')"
          BUNDLE="eguard-rules-${VERSION}.bundle.tar.zst"
          python3 -c "
          import json
          m = json.load(open('bundle/manifest.json'))
          try:
              coverage = json.load(open('bundle/coverage-metrics.json'))
          except Exception:
              coverage = {}
          try:
              regression = json.load(open('bundle/coverage-regression.json'))
          except Exception:
              regression = {}
          try:
              attack_coverage = json.load(open('bundle/attack-coverage.json'))
          except Exception:
              attack_coverage = {}
          try:
              attack_regression = json.load(open('bundle/attack-regression.json'))
          except Exception:
              attack_regression = {}
          try:
              attack_gap = json.load(open('bundle/attack-gap-burndown.json'))
          except Exception:
              attack_gap = {}
          try:
              attack_critical = json.load(open('bundle/attack-critical-technique-gate.json'))
          except Exception:
              attack_critical = {}
          try:
              attack_critical_regression = json.load(open('bundle/attack-critical-regression.json'))
          except Exception:
              attack_critical_regression = {}
          try:
              attack_critical_history = json.load(open('bundle/attack-critical-regression-history-summary.json'))
          except Exception:
              attack_critical_history = {}
          try:
              attack_critical_owner_streak = json.load(open('bundle/attack-critical-owner-streak-gate.json'))
          except Exception:
              attack_critical_owner_streak = {}
          try:
              attack_burndown = json.load(open('bundle/attack-burndown-scoreboard.json'))
          except Exception:
              attack_burndown = {}
          try:
              signature_ml_readiness = json.load(open('bundle/signature-ml-readiness.json'))
          except Exception:
              signature_ml_readiness = {}
          try:
              signature_ml_readiness_trend = json.load(open('bundle/signature-ml-readiness-trend-report.json'))
          except Exception:
              signature_ml_readiness_trend = {}
          try:
              signature_ml_corpus = json.load(open('bundle/signature-ml-training-corpus-summary.json'))
          except Exception:
              signature_ml_corpus = {}
          try:
              signature_ml_label_quality = json.load(open('bundle/signature-ml-label-quality-report.json'))
          except Exception:
              signature_ml_label_quality = {}
          try:
              signature_ml_feature_snapshot = json.load(open('bundle/signature-ml-feature-snapshot-report.json'))
          except Exception:
              signature_ml_feature_snapshot = {}
          try:
              signature_ml_offline_eval = json.load(open('bundle/signature-ml-offline-eval-report.json'))
          except Exception:
              signature_ml_offline_eval = {}
          try:
              signature_ml_offline_eval_trend = json.load(open('bundle/signature-ml-offline-eval-trend-report.json'))
          except Exception:
              signature_ml_offline_eval_trend = {}
          try:
              signature_ml_model_registry = json.load(open('bundle/signature-ml-model-registry.json'))
          except Exception:
              signature_ml_model_registry = {}
          src = m.get('sources', {})
          lines = [
              'Automated threat intelligence bundle  6-layer detection.',
              '',
              '## File Detection (YARA)',
              f'- **{m[\"yara_count\"]}** YARA rules (malware, exploits, C2 frameworks)',
              '',
              '## Log Detection (SIGMA)',
              f'- **{m[\"sigma_count\"]}** SIGMA rules (behavioral, emerging threats, hunting)',
              '',
              '## Network Detection (Suricata)',
              f'- **{m.get(\"suricata_count\", 0)}** Suricata rules (ET Open: C2, exploits, scanning)',
              '',
              '## Endpoint Behavior (Elastic)',
              f'- **{m.get(\"elastic_count\", 0)}** Elastic rules (Linux behavioral detection)',
              '',
              '## Indicators of Compromise',
              f'- IOC hashes: {m[\"ioc_hash_count\"]}',
              f'- IOC domains: {m[\"ioc_domain_count\"]}',
              f'- IOC IPs: {m[\"ioc_ip_count\"]}',
              '',
              '## Vulnerability Intelligence',
              f'- CVEs: {m[\"cve_count\"]}',
              f'- CISA KEV (actively exploited): {m.get(\"cve_kev_count\", \"n/a\")}',
              f'- EPSS-enriched: {m.get(\"cve_epss_count\", \"n/a\")}',
              '',
              '## Sources',
          ]

          if coverage:
              measured = coverage.get('measured', {})
              lines.extend([
                  '',
                  '## Signature DB Coverage Gate',
                  f'- Status: {coverage.get(\"status\", \"unknown\")}',
                  f'- Signature total: {measured.get(\"signature_total\", \"n/a\")}',
                  f'- Database total: {measured.get(\"database_total\", \"n/a\")}',
                  f'- YARA source count: {measured.get(\"yara_source_count\", \"n/a\")}',
                  f'- SIGMA source count: {measured.get(\"sigma_source_count\", \"n/a\")}',
              ])

          if regression:
              lines.extend([
                  '',
                  '## Coverage Regression Guard',
                  f'- Status: {regression.get(\"status\", \"unknown\")}',
              ])
              regressions = regression.get('regressions', [])
              if regressions:
                  lines.append(f'- Regressions: {\"; \".join(regressions)}')

          if attack_coverage:
              measured_attack = attack_coverage.get('measured', {})
              lines.extend([
                  '',
                  '## ATT&CK Coverage Gate',
                  f'- Status: {attack_coverage.get(\"status\", \"unknown\")}',
                  f'- Total techniques: {measured_attack.get(\"total_techniques\", \"n/a\")}',
                  f'- Total tactics: {measured_attack.get(\"total_tactics\", \"n/a\")}',
                  f'- Sigma rules with ATT&CK mapping: {measured_attack.get(\"sigma_rules_with_attack\", \"n/a\")}',
                  f'- Elastic rules with ATT&CK mapping: {measured_attack.get(\"elastic_rules_with_attack\", \"n/a\")}',
              ])

          if attack_regression:
              lines.extend([
                  '',
                  '## ATT&CK Regression Guard',
                  f'- Status: {attack_regression.get(\"status\", \"unknown\")}',
              ])
              attack_regressions = attack_regression.get('regressions', [])
              if attack_regressions:
                  lines.append(f'- Regressions: {\"; \".join(attack_regressions)}')

          if attack_gap:
              current_gap = attack_gap.get('current', {})
              burn_down = attack_gap.get('burn_down', {})
              missing_required = current_gap.get('missing_required_tactics', [])
              if not isinstance(missing_required, list):
                  missing_required = []
              lines.extend([
                  '',
                  '## ATT&CK Gap Burn-down',
                  f'- Status: {attack_gap.get(\"status\", \"unknown\")}',
                  f'- Technique gap remaining: {current_gap.get(\"technique_gap\", \"n/a\")}',
                  f'- Tactic gap remaining: {current_gap.get(\"tactic_gap\", \"n/a\")}',
                  f'- Missing required tactics: {len(missing_required)}',
                  f'- Technique gap reduced by: {burn_down.get(\"technique_gap_reduced_by\", \"n/a\")}',
                  f'- Tactic gap reduced by: {burn_down.get(\"tactic_gap_reduced_by\", \"n/a\")}',
              ])

          if attack_critical:
              measured_critical = attack_critical.get('measured', {})
              missing_required_critical = attack_critical.get('missing_required_techniques', [])
              if not isinstance(missing_required_critical, list):
                  missing_required_critical = []
              covered_ratio = measured_critical.get('covered_ratio', None)
              if isinstance(covered_ratio, (int, float)):
                  covered_ratio_display = f'{round(float(covered_ratio) * 100.0, 2)}%'
              else:
                  covered_ratio_display = 'n/a'
              lines.extend([
                  '',
                  '## Critical ATT&CK Technique Floor',
                  f'- Status: {attack_critical.get(\"status\", \"unknown\")}',
                  f'- Critical covered count: {measured_critical.get(\"covered_count\", \"n/a\")}',
                  f'- Critical covered ratio: {covered_ratio_display}',
                  f'- Critical missing count: {measured_critical.get(\"missing_count\", \"n/a\")}',
                  f'- Missing required critical techniques: {len(missing_required_critical)}',
              ])

          if attack_critical_regression:
              deltas = attack_critical_regression.get('deltas', {})
              if not isinstance(deltas, dict):
                  deltas = {}
              regressions = attack_critical_regression.get('regressions', [])
              if not isinstance(regressions, list):
                  regressions = []
              owner_p0_increase = deltas.get('owner_p0_increase_by_owner', {})
              if not isinstance(owner_p0_increase, dict):
                  owner_p0_increase = {}
              def _as_int(value):
                  try:
                      return int(value)
                  except Exception:
                      try:
                          return int(float(value))
                      except Exception:
                          return 0
              owner_p0_preview_items = sorted(
                  owner_p0_increase.items(),
                  key=lambda item: (-_as_int(item[1]), str(item[0])),
              )[:5]
              owner_p0_preview = '; '.join(
                  f'{owner}(+{increase})' for owner, increase in owner_p0_preview_items
              ) if owner_p0_preview_items else 'none'
              lines.extend([
                  '',
                  '## Critical ATT&CK Regression Guard',
                  f'- Status: {attack_critical_regression.get(\"status\", \"unknown\")}',
                  f'- Covered count delta: {deltas.get(\"covered_count_delta\", \"n/a\")}',
                  f'- Covered ratio delta: {deltas.get(\"covered_ratio_delta\", \"n/a\")}',
                  f'- Missing count delta: {deltas.get(\"missing_count_delta\", \"n/a\")}',
                  f'- Missing required count delta: {deltas.get(\"missing_required_count_delta\", \"n/a\")}',
                  f'- P0 uncovered count delta: {deltas.get(\"p0_uncovered_count_delta\", \"n/a\")}',
                  f'- Owner P0 regressions: {deltas.get(\"owner_p0_regression_count\", \"n/a\")}',
                  f'- Owner P0 increase preview (max 5): {owner_p0_preview}',
              ])
              if regressions:
                  lines.append(f'- Regressions: {"; ".join(regressions)}')

          if attack_critical_history:
              owner_totals = attack_critical_history.get('window_owner_p0_regression_totals', {})
              if not isinstance(owner_totals, dict):
                  owner_totals = {}
              owner_totals_preview = sorted(
                  owner_totals.items(),
                  key=lambda item: (-_as_int(item[1]), str(item[0])),
              )[:5]
              owner_totals_line = '; '.join(
                  f'{owner}({count})' for owner, count in owner_totals_preview
              ) if owner_totals_preview else 'none'
              lines.extend([
                  '',
                  '## Critical ATT&CK Regression History',
                  f'- History points: {attack_critical_history.get(\"history_points\", \"n/a\")}',
                  f'- Last-10 failures: {attack_critical_history.get(\"window_failures\", \"n/a\")}',
                  f'- Last-10 passes: {attack_critical_history.get(\"window_passes\", \"n/a\")}',
                  f'- Consecutive passes: {attack_critical_history.get(\"consecutive_passes\", \"n/a\")}',
                  f'- Last-10 owner regression totals (max 5): {owner_totals_line}',
              ])

          if attack_critical_owner_streak:
              violating = attack_critical_owner_streak.get('violating_owner_streaks', {})
              if not isinstance(violating, dict):
                  violating = {}
              violating_preview = sorted(
                  violating.items(),
                  key=lambda item: (-_as_int(item[1]), str(item[0])),
              )[:5]
              violating_line = '; '.join(
                  f'{owner}(streak={streak})' for owner, streak in violating_preview
              ) if violating_preview else 'none'
              current_regressing = attack_critical_owner_streak.get('current_regressing_owners', {})
              if not isinstance(current_regressing, dict):
                  current_regressing = {}
              current_preview = sorted(
                  current_regressing.items(),
                  key=lambda item: (-_as_int(item[1]), str(item[0])),
              )[:5]
              current_line = '; '.join(
                  f'{owner}(+{count})' for owner, count in current_preview
              ) if current_preview else 'none'
              lines.extend([
                  '',
                  '## Critical ATT&CK Owner Streak Guard',
                  f'- Status: {attack_critical_owner_streak.get(\"status\", \"unknown\")}',
                  f'- Evaluated points: {attack_critical_owner_streak.get(\"evaluated_points\", \"n/a\")}',
                  f'- Violating owner streaks: {len(violating)}',
                  f'- Violating preview (max 5): {violating_line}',
                  f'- Current regressing owners (max 5): {current_line}',
              ])

          if attack_burndown:
              trend = attack_burndown.get('trend', {})
              if not isinstance(trend, dict):
                  trend = {}
              uncovered = attack_burndown.get('uncovered_critical_techniques', [])
              if not isinstance(uncovered, list):
                  uncovered = []
              top_uncovered = attack_burndown.get('top_uncovered_critical_techniques', [])
              if not isinstance(top_uncovered, list):
                  top_uncovered = []
              newly_covered = trend.get('newly_covered', [])
              if not isinstance(newly_covered, list):
                  newly_covered = []
              newly_uncovered = trend.get('newly_uncovered', [])
              if not isinstance(newly_uncovered, list):
                  newly_uncovered = []
              top_preview = []
              for row in top_uncovered[:5]:
                  if not isinstance(row, dict):
                      continue
                  technique = str(row.get('technique', '')).strip().upper()
                  owner = str(row.get('owner', 'unassigned')).strip() or 'unassigned'
                  if technique:
                      top_preview.append(f'{technique}({owner})')
              top_preview_line = '; '.join(top_preview) if top_preview else 'none'
              lines.extend([
                  '',
                  '## ATT&CK Critical Burn-down Scoreboard',
                  f'- Critical coverage: {attack_burndown.get(\"critical_coverage_pct\", \"n/a\")}%',
                  f'- Uncovered critical techniques: {len(uncovered)}',
                  f'- Delta uncovered vs previous: {trend.get(\"delta_uncovered\", \"n/a\")}',
                  f'- Newly covered: {len(newly_covered)}',
                  f'- Newly uncovered: {len(newly_uncovered)}',
                  f'- Top uncovered critical (max 5): {top_preview_line}',
              ])

          if signature_ml_readiness:
              readiness_scores = signature_ml_readiness.get('scores', {})
              if not isinstance(readiness_scores, dict):
                  readiness_scores = {}
              lines.extend([
                  '',
                  '## Signature ML Readiness (Shadow)',
                  f'- Status: {signature_ml_readiness.get("status", "unknown")}',
                  f'- Mode: {signature_ml_readiness.get("mode", "unknown")}',
                  f'- Readiness tier: {signature_ml_readiness.get("readiness_tier", "unknown")}',
                  f'- Final score: {readiness_scores.get("final_score", "n/a")}',
                  f'- Previous final score: {readiness_scores.get("previous_final_score", "n/a")}',
                  f'- Score delta: {readiness_scores.get("score_delta", "n/a")}',
              ])

          if signature_ml_readiness_trend:
              trend_scores = signature_ml_readiness_trend.get('scores', {})
              if not isinstance(trend_scores, dict):
                  trend_scores = {}
              trend_alerts = signature_ml_readiness_trend.get('alerts', {})
              if not isinstance(trend_alerts, dict):
                  trend_alerts = {}
              trend_regressions = signature_ml_readiness_trend.get('regressions', [])
              if not isinstance(trend_regressions, list):
                  trend_regressions = []
              lines.extend([
                  '',
                  '## Signature ML Readiness Trend (Shadow)',
                  f'- Status: {signature_ml_readiness_trend.get("status", "unknown")}',
                  f'- History status: {signature_ml_readiness_trend.get("history_status", "unknown")}',
                  f'- Current final score: {trend_scores.get("current_final_score", "n/a")}',
                  f'- Previous final score: {trend_scores.get("previous_final_score", "n/a")}',
                  f'- Score drop: {trend_scores.get("score_drop", "n/a")}',
                  f'- Projected consecutive alerts: {trend_alerts.get("projected_consecutive_alerts", "n/a")}',
              ])
              if trend_regressions:
                  lines.append(f'- Regressions: {"; ".join(str(item) for item in trend_regressions)}')

          if signature_ml_corpus:
              corpus_measured = signature_ml_corpus.get('measured', {})
              if not isinstance(corpus_measured, dict):
                  corpus_measured = {}
              lines.extend([
                  '',
                  '## Signature ML Training Corpus',
                  f'- Status: {signature_ml_corpus.get("status", "unknown")}',
                  f'- Dataset mode: {signature_ml_corpus.get("dataset_mode", "unknown")}',
                  f'- Sample count: {corpus_measured.get("sample_count", "n/a")}',
                  f'- Adjudicated count: {corpus_measured.get("adjudicated_count", "n/a")}',
                  f'- Positive count: {corpus_measured.get("positive_count", "n/a")}',
                  f'- Negative count: {corpus_measured.get("negative_count", "n/a")}',
              ])

          if signature_ml_label_quality:
              label_measured = signature_ml_label_quality.get('measured', {})
              if not isinstance(label_measured, dict):
                  label_measured = {}
              lines.extend([
                  '',
                  '## Signature ML Label Quality (Shadow)',
                  f'- Status: {signature_ml_label_quality.get("status", "unknown")}',
                  f'- Adjudicated count: {label_measured.get("adjudicated_count", "n/a")}',
                  f'- Unresolved ratio: {label_measured.get("unresolved_ratio", "n/a")}',
                  f'- P95 label latency days: {label_measured.get("p95_label_latency_days", "n/a")}',
                  f'- Unique hosts: {label_measured.get("unique_hosts", "n/a")}',
                  f'- Unique rules: {label_measured.get("unique_rules", "n/a")}',
              ])

          if signature_ml_feature_snapshot:
              feature_measured = signature_ml_feature_snapshot.get('measured', {})
              if not isinstance(feature_measured, dict):
                  feature_measured = {}
              lines.extend([
                  '',
                  '## Signature ML Feature Snapshot (Shadow)',
                  f'- Status: {signature_ml_feature_snapshot.get("status", "unknown")}',
                  f'- Row count: {feature_measured.get("row_count", "n/a")}',
                  f'- Missing feature ratio: {feature_measured.get("missing_feature_ratio", "n/a")}',
                  f'- Temporal span days: {feature_measured.get("temporal_span_days", "n/a")}',
              ])

          if signature_ml_offline_eval:
              eval_metrics = signature_ml_offline_eval.get('metrics', {})
              if not isinstance(eval_metrics, dict):
                  eval_metrics = {}
              lines.extend([
                  '',
                  '## Signature ML Offline Eval (Shadow)',
                  f'- Status: {signature_ml_offline_eval.get("status", "unknown")}',
                  f'- Operating threshold: {eval_metrics.get("operating_threshold", "n/a")}',
                  f'- Precision: {eval_metrics.get("precision", "n/a")}',
                  f'- Recall: {eval_metrics.get("recall", "n/a")}',
                  f'- PR-AUC: {eval_metrics.get("pr_auc", "n/a")}',
                  f'- ROC-AUC: {eval_metrics.get("roc_auc", "n/a")}',
                  f'- Brier score: {eval_metrics.get("brier_score", "n/a")}',
                  f'- ECE: {eval_metrics.get("ece", "n/a")}',
              ])

          if signature_ml_offline_eval_trend:
              trend_alerts = signature_ml_offline_eval_trend.get('alerts', {})
              if not isinstance(trend_alerts, dict):
                  trend_alerts = {}
              trend_deltas = signature_ml_offline_eval_trend.get('deltas', {})
              if not isinstance(trend_deltas, dict):
                  trend_deltas = {}
              lines.extend([
                  '',
                  '## Signature ML Offline Eval Trend (Shadow)',
                  f'- Status: {signature_ml_offline_eval_trend.get("status", "unknown")}',
                  f'- History status: {signature_ml_offline_eval_trend.get("history_status", "unknown")}',
                  f'- Consecutive alerts: {trend_alerts.get("consecutive_alerts", "n/a")}',
                  f'- Window pass rate: {trend_alerts.get("window_pass_rate", "n/a")}',
                  f'- PR-AUC drop: {trend_deltas.get("pr_auc_drop", "n/a")}',
                  f'- ROC-AUC drop: {trend_deltas.get("roc_auc_drop", "n/a")}',
                  f'- Brier increase: {trend_deltas.get("brier_increase", "n/a")}',
                  f'- ECE increase: {trend_deltas.get("ece_increase", "n/a")}',
              ])

          if signature_ml_model_registry:
              registry_metrics = signature_ml_model_registry.get('offline_metrics', {})
              if not isinstance(registry_metrics, dict):
                  registry_metrics = {}
              lines.extend([
                  '',
                  '## Signature ML Model Registry (Shadow)',
                  f'- Status: {signature_ml_model_registry.get("status", "unknown")}',
                  f'- Model version: {signature_ml_model_registry.get("model_version", "unknown")}',
                  f'- Registry PR-AUC: {registry_metrics.get("pr_auc", "n/a")}',
                  f'- Registry ROC-AUC: {registry_metrics.get("roc_auc", "n/a")}',
              ])

          for category, names in src.items():
              names_str = ', '.join(names) if isinstance(names, list) else str(names)
              lines.append(f'- {category}: {names_str}')
          open('/tmp/release_notes.md', 'w').write('\n'.join(lines))
          "
          gh release create "rules-${VERSION}" \
            "${BUNDLE}" \
            "${BUNDLE}.sig" \
            "${BUNDLE}.pub.hex" \
            "bundle/coverage-metrics.json" \
            "bundle/coverage-regression.json" \
            "bundle/attack-coverage.json" \
            "bundle/attack-regression.json" \
            "bundle/attack-gap-burndown.json" \
            "bundle/attack-critical-technique-gate.json" \
            "bundle/attack-critical-regression.json" \
            "bundle/attack-critical-regression-history.ndjson" \
            "bundle/attack-critical-regression-history-summary.json" \
            "bundle/attack-critical-owner-streak-gate.json" \
            "bundle/attack-burndown-scoreboard.json" \
            "bundle/attack-burndown-scoreboard.md" \
            "bundle/signature-ml-readiness.json" \
            "bundle/signature-ml-readiness-trend.ndjson" \
            "bundle/signature-ml-readiness-trend-report.json" \
            "bundle/signature-ml-training-corpus-summary.json" \
            "bundle/signature-ml-signals.ndjson" \
            "bundle/signature-ml-label-quality-report.json" \
            "bundle/signature-ml-labels.ndjson" \
            "bundle/signature-ml-feature-snapshot-report.json" \
            "bundle/signature-ml-features.ndjson" \
            "bundle/signature-ml-feature-schema.json" \
            "bundle/signature-ml-model.json" \
            "bundle/signature-ml-model.json.sig" \
            "bundle/signature-ml-model.pub.pem" \
            "bundle/signature-ml-model-metadata.json" \
            "bundle/signature-ml-offline-eval-report.json" \
            "bundle/signature-ml-offline-eval-trend.ndjson" \
            "bundle/signature-ml-offline-eval-trend-report.json" \
            "bundle/signature-ml-model-registry.json" \
            --title "Rule Bundle ${VERSION}" \
            --notes-file /tmp/release_notes.md
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Release publish skipped (shadow mode)
        if: ${{ env.EGUARD_PUBLISH_BUNDLE != '1' || steps.publish_gate.outputs.ready != '1' }}
        run: |
          echo "Release publish skipped (shadow mode or gates not ready)."
